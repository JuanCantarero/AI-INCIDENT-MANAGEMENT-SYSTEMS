{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARIES\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache() # clean cache memory\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import ConcatDataset, TensorDataset, random_split, Dataset\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# To be able to apply transforms to a subset ... \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return self.subset.targets\n",
    "\n",
    "\n",
    "# Make sure I can run on my device's GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNLOAD THE DATASET INTO FOLDERS (URLs IN EXCEL - FOLDERS)\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Read the excel file\n",
    "df = pd.read_excel('incidencias_limpias_4.xlsx')\n",
    "\n",
    "# Create one folder per class\n",
    "clases = df['Clase'].unique()\n",
    "for clase in clases:\n",
    "    os.makedirs(clase, exist_ok=True)\n",
    "\n",
    "# Go through the dataframe and download the URLs\n",
    "for i, row in df.iterrows():\n",
    "    url = row['URL']\n",
    "    clase = row['Clase']\n",
    "    nombre = row['Nombre']\n",
    "    extension = url.split('.')[-1]\n",
    "    nombre_archivo = f\"{nombre}.{extension}\"\n",
    "    ruta_carpeta = clase\n",
    "    ruta_archivo = os.path.join(ruta_carpeta, nombre_archivo)\n",
    "    \n",
    "    # Download the image and save in its folder\n",
    "    response = requests.get(url)\n",
    "    with open(ruta_archivo, 'wb') as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING AND DATA AUGMENTATION\n",
    "\n",
    "# Link to the path folder that contains the images\n",
    "data_path = \"dataset\"\n",
    "\n",
    "# Image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop((500)),\n",
    "    transforms.Resize((260, 260)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4887, 0.4764, 0.4461],[0.2396, 0.2302, 0.2442])\n",
    "])\n",
    "\n",
    "# Create the main dataset from the folder containing all images and classes\n",
    "dataset = datasets.ImageFolder(root=data_path, transform=transform)\n",
    "\n",
    "# Divide the dataset into training and validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Define data augmentation transforms\n",
    "augmentation_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomHorizontalFlip(100),\n",
    "])\n",
    "\n",
    "# Create augmented dataset\n",
    "augmented_train_dataset = MyDataset(train_dataset, augmentation_transforms) # applies transformation to my subset\n",
    "\n",
    "# Concatenate datasets\n",
    "final_train_dataset = ConcatDataset([train_dataset, augmented_train_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT A RANDOM IMAGE \n",
    "\n",
    "# To make sure the datasets are right\n",
    "# Select a random image from the dataset\n",
    "image, label = random.choice(dataset) # use final_train_dataset to plot an image from the whole dataset (includes augmented)\n",
    "\n",
    "# plot an image via matplotlib\n",
    "plt.imshow(image.permute(1, 2, 0)) # traspose the image for it to be shown correctly\n",
    "plt.show()\n",
    "\n",
    "# show image class\n",
    "print(dataset.classes[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATION\n",
    "\n",
    "# Load pre-trained MobileNetV2 model. Train the las 3 conv. layers and the last F.C.\n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "# Define data loaders\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(final_train_dataset, batch_size=batch_size, shuffle=True) #train_dataset or final_train_dataset para aplicar data augmentation\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Freeze all layers except the last two\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.features[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.features[-2].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.features[-3].parameters():\n",
    "    param.requires_grad = True\n",
    "#for param in model.features[-4].parameters():\n",
    "#    param.requires_grad = True\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# Add dropout layer\n",
    "model.features.add_module('dropout', nn.Dropout(p=0.1))\n",
    "\n",
    "# Modify the last layer for our dataset\n",
    "num_classes = len(dataset.classes)\n",
    "in_features = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001,betas=(0.9,0.999),weight_decay=0.001)\n",
    "\n",
    "# defino el reductor de lr\n",
    "step_size = 4   # every \"step_size\" n of epochs\n",
    "gamma = 0.5     # lr multiplied by \"gamma\"\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 25\n",
    "train_losses, train_accs, val_losses, val_accs = [], [], [], [] # define them, will be plotted\n",
    "for epoch in range(num_epochs):\n",
    "    # Train the model for one epoch\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        train_correct += torch.sum(torch.argmax(outputs, dim=1) == labels)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            val_correct += torch.sum(torch.argmax(outputs, dim=1) == labels)\n",
    "\n",
    "    # Calculate metrics\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc = train_correct.float() / len(train_loader.dataset)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = val_correct.float() / len(val_loader.dataset)\n",
    "    \n",
    "    # Defined to plot loss curves\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    scheduler.step()\n",
    "    \n",
    "    \n",
    "# Plot the loss curves\n",
    "plt.plot(train_losses, label=\"Train\")\n",
    "plt.plot(val_losses, label=\"Validation\")\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.ylim(0,max(train_losses)*1.1)\n",
    "plt.savefig('loss_curves.eps', format='eps')\n",
    "plt.show()\n",
    "\n",
    "# Plot the accuracy curves\n",
    "cpu_train_accs = []\n",
    "for tensor in train_accs:\n",
    "    cpu_train_accs.append(tensor.cpu())\n",
    "plt.plot(cpu_train_accs, label=\"Train\")\n",
    "\n",
    "cpu_val_accs = []\n",
    "for tensor in val_accs:\n",
    "    cpu_val_accs.append(tensor.cpu())\n",
    "plt.plot(cpu_val_accs, label=\"Validation\")\n",
    "\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.ylim(0,max(cpu_train_accs)*1.1)\n",
    "plt.savefig('acc_curves.eps', format='eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFUSION MATRIX\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Get class names from dataset\n",
    "class_names = val_dataset.dataset.classes\n",
    "\n",
    "# Evaluate the model on the validation set and calculate the confusion matrix\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_true += labels.cpu().numpy().tolist()\n",
    "        y_pred += predicted.cpu().numpy().tolist()\n",
    "\n",
    "confusion = confusion_matrix(y_true, y_pred, labels=range(len(class_names)))\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=True, title='Matriz de confusión', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Adjust spacing between axes and figure borders\n",
    "    fig.subplots_adjust(left=0.15, bottom=0.15, right=0.95, top=0.95)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "# Plot the confusion matrix with class labels\n",
    "plot_confusion_matrix(confusion, classes=class_names, title='Matriz de confusión')\n",
    "plt.savefig('matx.eps', format='eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACCURACY DISTRIBUTION\n",
    "\n",
    "accuracy = np.diag(confusion) / np.sum(confusion, axis=1) * 100\n",
    "aciertos=np.diag(confusion)\n",
    "\n",
    "# Sort the labels and accuracy values in descending order\n",
    "sorted_indices = np.argsort(accuracy)\n",
    "sorted_labels = np.array(class_names)[sorted_indices]\n",
    "sorted_accuracy = accuracy[sorted_indices]\n",
    "sorted_aciertos = np.diag(confusion)[sorted_indices]\n",
    "\n",
    "# Create the graph \n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# Modify sorted_labels with sorted_accuracy\n",
    "ax.barh(sorted_labels, sorted_accuracy, color='steelblue') \n",
    "ax.set_xlabel('% de Acierto')  \n",
    "ax.tick_params(axis='x')  # Use axis='x' instead of axis='y'\n",
    "plt.setp(ax.get_yticklabels(), rotation=0, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "# Show number of right answers next to each bar\n",
    "for i, (acc, aciertos) in enumerate(zip(sorted_accuracy, sorted_aciertos)):\n",
    "    ax.text(acc, i, f'{aciertos}', ha='left', va='center')\n",
    "\n",
    "# Other configurations of te graph\n",
    "ax.set_ylabel('Etiqueta')  \n",
    "\n",
    "# Plot and save the graph\n",
    "plt.show()\n",
    "fig.savefig('pareto.eps', format='eps', dpi=300, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
